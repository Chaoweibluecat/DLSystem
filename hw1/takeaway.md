1. 广播
   1. 3 1  可以广播到 3 3 3 步骤如下
      1. 左边补1，对齐到 1 3 1 => 广播到 3 3 3
   2. 广播的梯度等于所有广播维度的求和(源通过多种路径影响到了最后loss 
source -> s1,s2,s3 -> loss grad: grad -> grad1,g2,g3 -> sum())
2. Summation:
   1. 注意默认降维的过程 sum (3,3,3) axis = (0,2) => output_shape = (3,)
   2. sum的梯度等于重新广播回求和的维度, sum可以看成多个Tensor求和, 加法的梯度就是原样返回
   3. 2中,你需要先将梯度reshape回(1,3,1),才可以准确的广播回去
3. MatMul: 需要注意 2 * 2 * 3 @ 3 * 4这种情况,框架应该要做自动广播, 在回传梯度的时候也需要匹配维度，做好summation(需要求和的维度总是从左开始)
4. 反向传播的起点总是 Loss那个Tensor,使用grad = 1,开始向前传播
   1. 假设Loss = (x-y)**2, 那么第一级Dl / dl =1, 传给（x-y）节点的loss就是 2* (x-y)
5. toposort: 使用
6. reverseAD: 使用了课件的简单mode 
   1. 反拓扑排序
   2. 每个节点出队后
      1. 从dict中取出自己所有上游传回的梯度求和
      2. 计算传递给所有出度的梯度，放入dict中
7. 反拓扑排序
   1. 简单的DFS后序+全局visited数组, 确保每个节点入队时所有出度都已经在结果队列中
   2. 结果的队列可以保证: 每个元素的出度的位置一定在自身之前(自己一定在inputs后面)
      1. 注: 正向拓扑序也就是前向传播的顺序
   3. reverse完: 每个元素的出度一定在自身之后(自己一定在inputs前面)
8. 反向传播过程不要用tensor做加减乘除。不要新增新的计算图节点